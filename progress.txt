*** 2019-08-14, Wednesday

A vanilla MLP on 2v2 has a mean score of ~1. It performs poorly likely because it doesn't know which targets and agents are dead.

*** 2019-08-15, Thursday

A vanilla MLP on 2v2, 16 steps has a mean score of ~1.15 with dead agents and targets masked at zero.

*** 2019-08-26, Monday

Caught a bug that was definitely stopping the GNN policies from training properly. The one node GNN test works and performs about as well as a vanilla MLP, but the first cut of a true GNN seriously underperforms.  Is there a flaw? Is the problem the "doesn't matter" actions?  Should the value vs policy graphs be more specialized?  Should I try a different training method?

I should really address the problem of number of time steps not incrementing, and adopt a more structured testing framework.

Implement variable agent/target numbers to help debugging. See how one agent performs.

*** 2019-08-27, Tuesday

What are the state_value and action_value actually being used for?
-- The action_value is not used by A2C.

Is the GPU being used?
-- Kind of.  We're basically doing GPU batches of 32, with a full round of CPU game compute between each round. How big a bottleneck is the CPU?

How should I feel about the three different losses?

How is the value loss defined? How big should it be?

What the the discounted rewards (and real rewards)? Why can the GNN sometimes get discounted rewards higher than the actual rewards?
-- I think the discounted rewards logged in TensorBoard are based on the estimated reward, not the true reward.

Determined that 'receivers' is not being filled correctly for batches > 1, and possibly for game sizes > 2. It looks like I've fixed this, and training is now stable.  All flavors of MLP / OneNode / GnnCoord joint and separate get about the same results after 30M steps, and haven't converged before then (reward 0.4).

*** 2019-08-28, Wednesday

Need to standardize record keeping for policies/simulations/test results.
Need to implement test reward during training.
Need to implement offline test evaluation.
Need to implement expert policy for score comparisons.
Need to implement game parameterization.
-- Added n_max_agents, r_capture
Need to implement randomized game sizes.
Need to fix training steps in TensorBoard.

Interested in:
  episode reward
  episode length

Conclusions:
    SubprocVecEnv vs. DummyVecEnv doesn't make much different on the laptop.

Observations:
    r_capture=0.1 is awfully small, but not totally impossible for n=1. Had very little luck with n=4.

*** 2019-08-29, Thursday

* DummyVec is slow.
* Increasing the number of steps doesn't much affect GPU use efficiency.
* Increasing the number of environments (even naively) improved GPU efficiency.

Throughput testing with this model:

# Miniature ICRA 2018 with msg_size = 8.
j = {}
j['policy'] = gnn_fwd.GnnFwd
j['policy_param'] = {
    'input_feat_layers':    (64,64),
    'feat_agg_layers':      (),
    'msg_enc_layers':       (64,64),
    'msg_size':             8,
    'msg_dec_layers':       (64,64),
    'msg_agg_layers':       (64,64),
    'pi_head_layers':       (),
    'vf_local_head_layers': (),
    'vf_global_head_layers':()}
j['name'] = j['policy'].policy_param_string(j['policy_param'])
jobs.append(j)

>> n_env = 16

n_steps, S-fps
8,  4200
16, 4640
32, 4500
64, 4900

Varying n_steps doesn't much affect throughput.

>> n_steps = 32

n_env, S-fps, D-fps
8,   2750
16,  4640
32,  6150
64,  6600
128, 8300

Increasing n_env increased throughput, even past n_cpu
